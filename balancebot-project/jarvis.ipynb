{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gopi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from baselines import deepq\n",
    "import balance_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: Environment '<class 'balance_bot.envs.balancebot_env.BalancebotEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"balancebot-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "# env.step(env.action_space.sample())\n",
    "for _ in range(100):\n",
    "#     env.render(mode='human')\n",
    "    state, reward, done , info = env.step(env.action_space.sample())\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "#         print(rewards[:10])\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21342179, -2.00501655, -5.5       ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self , learning_rate=0.001,\n",
    "                state_size=3, \n",
    "                action_size=9,\n",
    "                hidden_size=16,\n",
    "                name='QNetwork'):\n",
    "        # state inputs to the Q-Network\n",
    "        # https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to latte choose the Q-Value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32 , [None] , name=\"actions\") # action value is 0 or 1 , one_hot take int values\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None] , name=\"targetQ\")\n",
    "            \n",
    "            # ReLU hidden layer\n",
    "            # See tf.layers.dense() tf.contrib.layers.fully_connected()\n",
    "#             self.fc1 = tf.contrib.layers.fully_connected(self.inputs_ , hidden_size , weights_initializer=ttf.contrib.layers.xavier_initializer ,weights_regularizer=None) # activation relu applied by default \n",
    "#             self.fc2 = tf.contrib.layers.fully_connected(self.fc1 , hidden_size, weights_initializer=tf.contrib.layers.xavier_initializer ,weights_regularizer=None)\n",
    "            self.fc1 = tf.layers.dense(self.inputs_ , hidden_size , activation=tf.nn.relu , kernel_initializer=tf.contrib.layers.xavier_initializer() ) #, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "            self.hc1 = tf.layers.dense(self.fc1 , hidden_size , activation=tf.nn.relu , kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc2 = tf.layers.dense(self.hc1, hidden_size , activation=tf.nn.relu , kernel_initializer=tf.contrib.layers.xavier_initializer()) # , kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "            # Linear output layer\n",
    "#             self.output = tf.contrib.layers.fully_connected(self.fc2 , action_size , activation_fn=None)\n",
    "            self.output = tf.layers.dense(self.fc2 , action_size , activation=None)\n",
    "            \n",
    "            \n",
    "            ## Train with loss (targetQ - Q)^2\n",
    "            # output has length 9, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output , one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self , max_size= 1000):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        index = np.random.choice(np.arange(len(self.buffer)),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        return [self.buffer[idx] for idx in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 500          # max number of episodes to learn from\n",
    "max_steps = 400              # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 0.3            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 50000            # memory capacity\n",
    "batch_size = 64                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name=\"main\" , hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state , reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for i in range(pretrain_length):\n",
    "    #env.render()\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    \n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state,action, reward , next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: -1.9304019096194391 Training loss: 0.0498 Explore Prob: 0.2979\n",
      "Episode: 2 Total reward: 1.6517773706246033 Training loss: 0.1089 Explore Prob: 0.2962\n",
      "Episode: 3 Total reward: 2.3206454823045255 Training loss: 0.9538 Explore Prob: 0.2932\n",
      "Episode: 4 Total reward: -0.2071199192477486 Training loss: 7.6148 Explore Prob: 0.2918\n",
      "Episode: 5 Total reward: 3.5944633554509076 Training loss: 13.3642 Explore Prob: 0.2896\n",
      "Episode: 6 Total reward: 1.4241389814703709 Training loss: 321.7290 Explore Prob: 0.2879\n",
      "Episode: 7 Total reward: 4.050348837807045 Training loss: 35.2715 Explore Prob: 0.2855\n",
      "Episode: 8 Total reward: 1.2340351485112924 Training loss: 136.7582 Explore Prob: 0.2838\n",
      "Episode: 9 Total reward: 0.41294598280880934 Training loss: 1490.4381 Explore Prob: 0.2823\n",
      "Episode: 10 Total reward: -0.4667434834728469 Training loss: 35.2275 Explore Prob: 0.2790\n",
      "Episode: 11 Total reward: 2.991322137921856 Training loss: 1034.0236 Explore Prob: 0.2770\n",
      "Episode: 12 Total reward: -0.4115630065451964 Training loss: 35.0332 Explore Prob: 0.2757\n",
      "Episode: 13 Total reward: 1.3580364559806777 Training loss: 230.4236 Explore Prob: 0.2741\n",
      "Episode: 14 Total reward: -0.038040005856044196 Training loss: 13.8878 Explore Prob: 0.2728\n",
      "Episode: 15 Total reward: 0.8951850321106327 Training loss: 166.1118 Explore Prob: 0.2713\n",
      "Episode: 16 Total reward: 3.165896390185905 Training loss: 23.2809 Explore Prob: 0.2693\n",
      "Episode: 17 Total reward: 1.2908742172663095 Training loss: 5.9512 Explore Prob: 0.2677\n",
      "Episode: 18 Total reward: 0.2789151964676909 Training loss: 187.8330 Explore Prob: 0.2664\n",
      "Episode: 19 Total reward: 0.9135459652120477 Training loss: 5.2362 Explore Prob: 0.2649\n",
      "Episode: 20 Total reward: 0.6786811987676745 Training loss: 131.0401 Explore Prob: 0.2636\n",
      "Episode: 21 Total reward: 0.8945810598851126 Training loss: 10.7217 Explore Prob: 0.2622\n",
      "Episode: 22 Total reward: 2.5600094028767586 Training loss: 173.2103 Explore Prob: 0.2598\n",
      "Episode: 23 Total reward: 3.2260382981469924 Training loss: 2.3299 Explore Prob: 0.2578\n",
      "Episode: 24 Total reward: 1.7702961714898697 Training loss: 145.4897 Explore Prob: 0.2562\n",
      "Episode: 25 Total reward: 6.625745062012337 Training loss: 6.0861 Explore Prob: 0.2537\n",
      "Episode: 26 Total reward: -0.3146528532618137 Training loss: 44.6928 Explore Prob: 0.2525\n",
      "Episode: 27 Total reward: 1.1501481072961193 Training loss: 1.4321 Explore Prob: 0.2510\n",
      "Episode: 28 Total reward: 3.9494155524434857 Training loss: 1.9572 Explore Prob: 0.2490\n",
      "Episode: 29 Total reward: 8.137477457272222 Training loss: 47.2926 Explore Prob: 0.2461\n",
      "Episode: 30 Total reward: 4.703868077796736 Training loss: 7.9956 Explore Prob: 0.2441\n",
      "Episode: 31 Total reward: 6.720715684280675 Training loss: 49.9310 Explore Prob: 0.2415\n",
      "Episode: 32 Total reward: 4.85863300399581 Training loss: 0.4800 Explore Prob: 0.2392\n",
      "Episode: 33 Total reward: 7.3991219367721035 Training loss: 8.8472 Explore Prob: 0.2364\n",
      "Episode: 34 Total reward: 8.41713904362302 Training loss: 1.4805 Explore Prob: 0.2334\n",
      "Episode: 35 Total reward: 2.4255571696324503 Training loss: 1.1528 Explore Prob: 0.2293\n",
      "Episode: 36 Total reward: 3.598245272884676 Training loss: 20.0785 Explore Prob: 0.2276\n",
      "Episode: 37 Total reward: 8.341834554960908 Training loss: 0.8137 Explore Prob: 0.2248\n",
      "Episode: 38 Total reward: 2.318203850867476 Training loss: 0.4492 Explore Prob: 0.2233\n",
      "Episode: 39 Total reward: 7.94279248895764 Training loss: 4.0286 Explore Prob: 0.2208\n",
      "Episode: 40 Total reward: 8.828090901446021 Training loss: 3.4222 Explore Prob: 0.2181\n",
      "Episode: 41 Total reward: 4.9591404911546535 Training loss: 0.7813 Explore Prob: 0.2162\n",
      "Episode: 42 Total reward: 6.410111373899894 Training loss: 0.3307 Explore Prob: 0.2141\n",
      "Episode: 43 Total reward: 4.86710325863039 Training loss: 0.7268 Explore Prob: 0.2122\n",
      "Episode: 44 Total reward: 4.177163308748198 Training loss: 1.6722 Explore Prob: 0.2106\n",
      "Episode: 45 Total reward: 10.434212786024236 Training loss: 1.5781 Explore Prob: 0.2076\n",
      "Episode: 46 Total reward: 5.601937617968038 Training loss: 0.2637 Explore Prob: 0.2057\n",
      "Episode: 47 Total reward: 5.519566564240261 Training loss: 7.1763 Explore Prob: 0.2039\n",
      "Episode: 48 Total reward: 3.1430935591471036 Training loss: 2.4814 Explore Prob: 0.2025\n",
      "Episode: 49 Total reward: 4.426892466355162 Training loss: 0.5570 Explore Prob: 0.2009\n",
      "Episode: 50 Total reward: 4.0606910511710375 Training loss: 0.4777 Explore Prob: 0.1994\n",
      "Episode: 51 Total reward: 3.1664238231576713 Training loss: 0.2572 Explore Prob: 0.1980\n",
      "Episode: 52 Total reward: 4.617446389166984 Training loss: 2.5445 Explore Prob: 0.1964\n",
      "Episode: 53 Total reward: 5.63182391556479 Training loss: 0.2815 Explore Prob: 0.1945\n",
      "Episode: 54 Total reward: 4.499640958271036 Training loss: 6.5057 Explore Prob: 0.1928\n",
      "Episode: 55 Total reward: 6.325077424738446 Training loss: 2.1251 Explore Prob: 0.1909\n",
      "Episode: 56 Total reward: 5.003113456242091 Training loss: 2.0050 Explore Prob: 0.1893\n",
      "Episode: 57 Total reward: 5.017118679489975 Training loss: 0.1968 Explore Prob: 0.1876\n",
      "Episode: 58 Total reward: 6.182703221127398 Training loss: 1.8185 Explore Prob: 0.1858\n",
      "Episode: 59 Total reward: 5.0924322424009025 Training loss: 1.8841 Explore Prob: 0.1842\n",
      "Episode: 60 Total reward: 6.032352352759848 Training loss: 3.8755 Explore Prob: 0.1824\n",
      "Episode: 61 Total reward: 4.234171787035117 Training loss: 3.5081 Explore Prob: 0.1809\n",
      "Episode: 62 Total reward: 2.110582234190002 Training loss: 2.0647 Explore Prob: 0.1798\n",
      "Episode: 63 Total reward: 4.291156435764556 Training loss: 0.1920 Explore Prob: 0.1783\n",
      "Episode: 64 Total reward: 5.569979723976604 Training loss: 2.7547 Explore Prob: 0.1766\n",
      "Episode: 65 Total reward: 2.038902497216349 Training loss: 0.2866 Explore Prob: 0.1755\n",
      "Episode: 66 Total reward: 1.4716257745505048 Training loss: 0.2387 Explore Prob: 0.1745\n",
      "Episode: 67 Total reward: 2.5940542496410197 Training loss: 0.3776 Explore Prob: 0.1733\n",
      "Episode: 68 Total reward: 2.994758023099082 Training loss: 2.4377 Explore Prob: 0.1722\n",
      "Episode: 69 Total reward: 2.147765404313282 Training loss: 1.6115 Explore Prob: 0.1711\n",
      "Episode: 70 Total reward: 3.0763011108575653 Training loss: 0.1342 Explore Prob: 0.1699\n",
      "Episode: 71 Total reward: 4.470926116942353 Training loss: 0.1950 Explore Prob: 0.1685\n",
      "Episode: 72 Total reward: 3.8991314180947874 Training loss: 0.2062 Explore Prob: 0.1671\n",
      "Episode: 73 Total reward: 4.165806816111233 Training loss: 0.1844 Explore Prob: 0.1658\n",
      "Episode: 74 Total reward: 3.5934451494045296 Training loss: 1.3312 Explore Prob: 0.1646\n",
      "Episode: 75 Total reward: 3.3757898046813826 Training loss: 0.8416 Explore Prob: 0.1634\n",
      "Episode: 76 Total reward: 2.660146615317293 Training loss: 2.2442 Explore Prob: 0.1624\n",
      "Episode: 77 Total reward: 2.894494529317348 Training loss: 1.0244 Explore Prob: 0.1613\n",
      "Episode: 78 Total reward: 2.2084548681940857 Training loss: 0.2071 Explore Prob: 0.1603\n",
      "Episode: 79 Total reward: 3.6072129862420557 Training loss: 0.6748 Explore Prob: 0.1591\n",
      "Episode: 80 Total reward: 3.6766920930623015 Training loss: 0.8934 Explore Prob: 0.1579\n",
      "Episode: 81 Total reward: 3.41243805489789 Training loss: 0.9645 Explore Prob: 0.1568\n",
      "Episode: 82 Total reward: 4.87540056919505 Training loss: 0.1657 Explore Prob: 0.1555\n",
      "Episode: 83 Total reward: 3.2661415474779067 Training loss: 0.1488 Explore Prob: 0.1544\n",
      "Episode: 84 Total reward: 3.2941526643700207 Training loss: 0.2082 Explore Prob: 0.1533\n",
      "Episode: 85 Total reward: 4.1402633749819495 Training loss: 1.6390 Explore Prob: 0.1521\n",
      "Episode: 86 Total reward: 3.8038393566701667 Training loss: 1.8491 Explore Prob: 0.1509\n",
      "Episode: 87 Total reward: 5.0109073567652445 Training loss: 2.4157 Explore Prob: 0.1496\n",
      "Episode: 88 Total reward: 2.0912347512584586 Training loss: 0.2020 Explore Prob: 0.1487\n",
      "Episode: 89 Total reward: 2.104374908210039 Training loss: 0.2137 Explore Prob: 0.1478\n",
      "Episode: 90 Total reward: 2.9031484023937217 Training loss: 0.2075 Explore Prob: 0.1468\n",
      "Episode: 91 Total reward: 3.517885049059287 Training loss: 0.3633 Explore Prob: 0.1457\n",
      "Episode: 92 Total reward: 2.062088941523737 Training loss: 0.1208 Explore Prob: 0.1449\n",
      "Episode: 93 Total reward: 2.127947312486329 Training loss: 0.3719 Explore Prob: 0.1440\n",
      "Episode: 94 Total reward: 1.960433384049136 Training loss: 0.2372 Explore Prob: 0.1431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 95 Total reward: 2.181124789651172 Training loss: 0.1855 Explore Prob: 0.1423\n",
      "Episode: 96 Total reward: 3.039610404422819 Training loss: 0.2441 Explore Prob: 0.1413\n",
      "Episode: 97 Total reward: 1.433034967479469 Training loss: 2.2274 Explore Prob: 0.1405\n",
      "Episode: 98 Total reward: 1.9552866662357138 Training loss: 0.1887 Explore Prob: 0.1397\n",
      "Episode: 99 Total reward: 1.586182687703453 Training loss: 0.5400 Explore Prob: 0.1389\n",
      "Episode: 100 Total reward: 2.5664580371268264 Training loss: 0.1517 Explore Prob: 0.1380\n",
      "Episode: 101 Total reward: 2.1217924640221706 Training loss: 0.5240 Explore Prob: 0.1372\n",
      "Episode: 102 Total reward: 0.1491354532856637 Training loss: 0.3576 Explore Prob: 0.1365\n",
      "Episode: 103 Total reward: 1.1181994834697901 Training loss: 0.2783 Explore Prob: 0.1358\n",
      "Episode: 104 Total reward: 2.504545494605659 Training loss: 0.8302 Explore Prob: 0.1350\n",
      "Episode: 105 Total reward: 8.094847182770206 Training loss: 0.1709 Explore Prob: 0.1328\n",
      "Episode: 106 Total reward: 3.7296621153523066 Training loss: 0.2501 Explore Prob: 0.1307\n",
      "Episode: 107 Total reward: 6.743129235295851 Training loss: 0.2398 Explore Prob: 0.1291\n",
      "Episode: 108 Total reward: -4.090727512878848 Training loss: 0.1168 Explore Prob: 0.1274\n",
      "Episode: 109 Total reward: 4.4616298227106475 Training loss: 0.1152 Explore Prob: 0.1264\n",
      "Episode: 110 Total reward: -10.399711089929784 Training loss: 0.0679 Explore Prob: 0.1250\n",
      "Episode: 111 Total reward: -2.323045568829201 Training loss: 0.1082 Explore Prob: 0.1237\n",
      "Episode: 112 Total reward: -1.4778265920733136 Training loss: 0.1168 Explore Prob: 0.1228\n",
      "Episode: 113 Total reward: 1.9867154165272303 Training loss: 0.2045 Explore Prob: 0.1219\n",
      "Episode: 114 Total reward: -1.5720603913833198 Training loss: 0.1055 Explore Prob: 0.1207\n",
      "Episode: 115 Total reward: 2.675245434070422 Training loss: 0.1743 Explore Prob: 0.1199\n",
      "Episode: 116 Total reward: 1.262835502042878 Training loss: 0.7832 Explore Prob: 0.1192\n",
      "Episode: 117 Total reward: 3.449366258657409 Training loss: 0.1941 Explore Prob: 0.1180\n",
      "Episode: 118 Total reward: 5.990530981576637 Training loss: 0.1473 Explore Prob: 0.1166\n",
      "Episode: 119 Total reward: 3.135040337755429 Training loss: 0.2043 Explore Prob: 0.1155\n",
      "Episode: 120 Total reward: 6.394840878406365 Training loss: 0.1627 Explore Prob: 0.1139\n",
      "Episode: 121 Total reward: 7.363672049265733 Training loss: 0.1601 Explore Prob: 0.1123\n",
      "Episode: 122 Total reward: 5.737527172662645 Training loss: 0.1002 Explore Prob: 0.1110\n",
      "Episode: 123 Total reward: 4.919033874723405 Training loss: 0.1067 Explore Prob: 0.1096\n",
      "Episode: 124 Total reward: -2.589146118758285 Training loss: 0.4769 Explore Prob: 0.1083\n",
      "Episode: 125 Total reward: 4.854815671390356 Training loss: 0.2081 Explore Prob: 0.1068\n",
      "Episode: 126 Total reward: 5.713929540777726 Training loss: 0.1143 Explore Prob: 0.1053\n",
      "Episode: 127 Total reward: 10.091695175087569 Training loss: 0.0831 Explore Prob: 0.1037\n",
      "Episode: 128 Total reward: 15.277829460712784 Training loss: 0.2217 Explore Prob: 0.1017\n",
      "Episode: 129 Total reward: 3.2542634775082666 Training loss: 0.1284 Explore Prob: 0.0999\n",
      "Episode: 130 Total reward: 3.601838084720893 Training loss: 0.3138 Explore Prob: 0.0983\n",
      "Episode: 131 Total reward: 6.057250430880557 Training loss: 0.0890 Explore Prob: 0.0967\n",
      "Episode: 132 Total reward: 6.659719704713499 Training loss: 0.0782 Explore Prob: 0.0956\n",
      "Episode: 133 Total reward: -0.27602418665553013 Training loss: 0.2726 Explore Prob: 0.0939\n",
      "Episode: 134 Total reward: 6.641089022977491 Training loss: 0.1332 Explore Prob: 0.0925\n",
      "Episode: 135 Total reward: 6.562626379952054 Training loss: 0.1559 Explore Prob: 0.0908\n",
      "Episode: 136 Total reward: 7.962163477251089 Training loss: 0.2108 Explore Prob: 0.0891\n",
      "Episode: 137 Total reward: 4.652036170239029 Training loss: 0.1331 Explore Prob: 0.0883\n",
      "Episode: 138 Total reward: 5.572451526049837 Training loss: 0.2274 Explore Prob: 0.0874\n",
      "Episode: 139 Total reward: 2.8162699629556336 Training loss: 0.1178 Explore Prob: 0.0867\n",
      "Episode: 140 Total reward: 6.094855668002999 Training loss: 0.9433 Explore Prob: 0.0855\n",
      "Episode: 141 Total reward: 4.169049530795953 Training loss: 0.1995 Explore Prob: 0.0849\n",
      "Episode: 142 Total reward: 2.52349480323412 Training loss: 0.1393 Explore Prob: 0.0844\n",
      "Episode: 143 Total reward: 1.0349440087760298 Training loss: 0.3270 Explore Prob: 0.0828\n",
      "Episode: 144 Total reward: -0.641888567990426 Training loss: 0.3925 Explore Prob: 0.0822\n",
      "Episode: 145 Total reward: -20.909836034413097 Training loss: 0.4522 Explore Prob: 0.0806\n",
      "Episode: 146 Total reward: 6.596357806967034 Training loss: 0.2690 Explore Prob: 0.0796\n",
      "Episode: 148 Total reward: -21.969055486635526 Training loss: 0.3731 Explore Prob: 0.0764\n",
      "Episode: 149 Total reward: 3.875327868546506 Training loss: 1.5490 Explore Prob: 0.0757\n",
      "Episode: 151 Total reward: 5.317152952969321 Training loss: 1.8682 Explore Prob: 0.0717\n",
      "Episode: 152 Total reward: 9.927694697234703 Training loss: 0.4293 Explore Prob: 0.0704\n",
      "Episode: 153 Total reward: 6.964052911144395 Training loss: 0.6730 Explore Prob: 0.0697\n",
      "Episode: 154 Total reward: 4.261178244687151 Training loss: 11.6029 Explore Prob: 0.0688\n",
      "Episode: 155 Total reward: -27.091592403540222 Training loss: 1.7467 Explore Prob: 0.0674\n",
      "Episode: 157 Total reward: 5.1940341208604925 Training loss: 3.3957 Explore Prob: 0.0639\n",
      "Episode: 158 Total reward: 0.3712654698955764 Training loss: 1.8051 Explore Prob: 0.0629\n",
      "Episode: 159 Total reward: -5.116543566514031 Training loss: 0.4385 Explore Prob: 0.0611\n",
      "Episode: 161 Total reward: -5.1867978185045365 Training loss: 7.1261 Explore Prob: 0.0590\n",
      "Episode: 162 Total reward: 1.770014079841261 Training loss: 5.3698 Explore Prob: 0.0585\n",
      "Episode: 164 Total reward: 2.869403156827792 Training loss: 1.0029 Explore Prob: 0.0555\n",
      "Episode: 168 Total reward: -0.6000126052258391 Training loss: 3.2447 Explore Prob: 0.0503\n",
      "Episode: 169 Total reward: 8.42132912326242 Training loss: 18.9700 Explore Prob: 0.0489\n",
      "Episode: 170 Total reward: -6.116060637944033 Training loss: 9.2572 Explore Prob: 0.0479\n",
      "Episode: 171 Total reward: 3.9759805758265783 Training loss: 117.0921 Explore Prob: 0.0468\n",
      "Episode: 172 Total reward: -0.639798604702036 Training loss: 10999.0342 Explore Prob: 0.0466\n",
      "Episode: 173 Total reward: -1.4691797899952754 Training loss: 13698.7754 Explore Prob: 0.0465\n",
      "Episode: 174 Total reward: -1.3201515416162315 Training loss: 146155.6875 Explore Prob: 0.0463\n",
      "Episode: 175 Total reward: -1.5004678271440257 Training loss: 9322180.0000 Explore Prob: 0.0462\n",
      "Episode: 176 Total reward: -1.243256128915829 Training loss: 17686976.0000 Explore Prob: 0.0461\n",
      "Episode: 177 Total reward: -1.2870647896814231 Training loss: 65950072.0000 Explore Prob: 0.0459\n",
      "Episode: 178 Total reward: -1.4634029262819213 Training loss: 35023464.0000 Explore Prob: 0.0458\n",
      "Episode: 179 Total reward: -1.304156621643911 Training loss: 159314656.0000 Explore Prob: 0.0457\n",
      "Episode: 180 Total reward: -1.4691797899952754 Training loss: 85191840.0000 Explore Prob: 0.0455\n",
      "Episode: 181 Total reward: -1.3613853862662058 Training loss: 103090264.0000 Explore Prob: 0.0454\n",
      "Episode: 182 Total reward: -1.2610193704484232 Training loss: 111643472.0000 Explore Prob: 0.0453\n",
      "Episode: 183 Total reward: -1.5865017944229491 Training loss: 4254441216.0000 Explore Prob: 0.0451\n",
      "Episode: 184 Total reward: -1.4691797899952754 Training loss: 177570464.0000 Explore Prob: 0.0450\n",
      "Episode: 185 Total reward: -1.4390309971762554 Training loss: 72260328.0000 Explore Prob: 0.0449\n",
      "Episode: 186 Total reward: -1.5626652286515221 Training loss: 923803840.0000 Explore Prob: 0.0447\n",
      "Episode: 187 Total reward: -1.5202628081019736 Training loss: 1769482240.0000 Explore Prob: 0.0446\n",
      "Episode: 188 Total reward: -1.4333220534915239 Training loss: 182954240.0000 Explore Prob: 0.0445\n",
      "Episode: 189 Total reward: -1.260025898861787 Training loss: 367026624.0000 Explore Prob: 0.0443\n",
      "Episode: 190 Total reward: -1.4790290189364503 Training loss: 205766752.0000 Explore Prob: 0.0442\n",
      "Episode: 191 Total reward: -1.3282379799271886 Training loss: 9579060224.0000 Explore Prob: 0.0441\n",
      "Episode: 192 Total reward: -1.413958155743293 Training loss: 254823792.0000 Explore Prob: 0.0439\n",
      "Episode: 193 Total reward: -1.1400398829045042 Training loss: 6500168192.0000 Explore Prob: 0.0438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 194 Total reward: -1.3419055896778365 Training loss: 154127856.0000 Explore Prob: 0.0437\n",
      "Episode: 195 Total reward: -1.4634029262819213 Training loss: 71737904.0000 Explore Prob: 0.0436\n",
      "Episode: 196 Total reward: -1.5626652286515221 Training loss: 328782144.0000 Explore Prob: 0.0434\n",
      "Episode: 197 Total reward: -1.1381883777914967 Training loss: 2299502592.0000 Explore Prob: 0.0433\n",
      "Episode: 198 Total reward: -1.491697335480214 Training loss: 174475120.0000 Explore Prob: 0.0432\n",
      "Episode: 199 Total reward: -1.4755767520191987 Training loss: 6299365376.0000 Explore Prob: 0.0431\n",
      "Episode: 200 Total reward: -1.1553661485889277 Training loss: 2214041088.0000 Explore Prob: 0.0429\n",
      "Episode: 201 Total reward: -1.3339853276187457 Training loss: 122427976.0000 Explore Prob: 0.0428\n",
      "Episode: 202 Total reward: -1.4291609553191216 Training loss: 165366368.0000 Explore Prob: 0.0427\n",
      "Episode: 203 Total reward: -1.2523274866550627 Training loss: 2802337280.0000 Explore Prob: 0.0426\n",
      "Episode: 204 Total reward: -1.2834436254666128 Training loss: 169141536.0000 Explore Prob: 0.0424\n",
      "Episode: 205 Total reward: -1.267071440858495 Training loss: 113399960.0000 Explore Prob: 0.0423\n",
      "Episode: 206 Total reward: -1.564019273897355 Training loss: 9910564864.0000 Explore Prob: 0.0422\n",
      "Episode: 207 Total reward: -1.473038896584524 Training loss: 125225632.0000 Explore Prob: 0.0421\n",
      "Episode: 208 Total reward: -1.5238742339684541 Training loss: 114883536.0000 Explore Prob: 0.0420\n",
      "Episode: 209 Total reward: -1.0813577770233078 Training loss: 130697840.0000 Explore Prob: 0.0418\n",
      "Episode: 210 Total reward: 0.39714755589280853 Training loss: 99582992.0000 Explore Prob: 0.0417\n",
      "Episode: 211 Total reward: -0.025678312854070573 Training loss: 102683568.0000 Explore Prob: 0.0415\n",
      "Episode: 212 Total reward: -1.4144341044750057 Training loss: 146881856.0000 Explore Prob: 0.0414\n",
      "Episode: 213 Total reward: -0.9798541436049157 Training loss: 73890496.0000 Explore Prob: 0.0413\n",
      "Episode: 214 Total reward: -1.211531429155295 Training loss: 7844527104.0000 Explore Prob: 0.0411\n",
      "Episode: 215 Total reward: -1.1764131748611353 Training loss: 68838000.0000 Explore Prob: 0.0410\n",
      "Episode: 216 Total reward: -1.2828807636532213 Training loss: 63648512.0000 Explore Prob: 0.0409\n",
      "Episode: 217 Total reward: -1.3448424457461428 Training loss: 7764429312.0000 Explore Prob: 0.0408\n",
      "Episode: 218 Total reward: -1.4081401324627314 Training loss: 290366944.0000 Explore Prob: 0.0407\n",
      "Episode: 219 Total reward: -1.448058643658757 Training loss: 98804584.0000 Explore Prob: 0.0406\n",
      "Episode: 220 Total reward: -0.6214393328637572 Training loss: 82807488.0000 Explore Prob: 0.0404\n",
      "Episode: 221 Total reward: 0.3113944921880087 Training loss: 1456499968.0000 Explore Prob: 0.0403\n",
      "Episode: 222 Total reward: -0.8125386789268392 Training loss: 71084928.0000 Explore Prob: 0.0401\n",
      "Episode: 223 Total reward: 0.1322547062215644 Training loss: 97065928.0000 Explore Prob: 0.0400\n",
      "Episode: 224 Total reward: -1.362740081580481 Training loss: 52682504.0000 Explore Prob: 0.0399\n",
      "Episode: 225 Total reward: -1.3214403976574107 Training loss: 976047936.0000 Explore Prob: 0.0398\n",
      "Episode: 226 Total reward: -1.1211148606434223 Training loss: 63469680.0000 Explore Prob: 0.0397\n",
      "Episode: 227 Total reward: 0.38503289175604544 Training loss: 63661168.0000 Explore Prob: 0.0395\n",
      "Episode: 228 Total reward: 0.7770819899627455 Training loss: 91346704.0000 Explore Prob: 0.0393\n",
      "Episode: 229 Total reward: 0.5842307053346457 Training loss: 5753914880.0000 Explore Prob: 0.0392\n",
      "Episode: 230 Total reward: -1.2435679236285546 Training loss: 29255432.0000 Explore Prob: 0.0391\n",
      "Episode: 231 Total reward: -0.4041978612414614 Training loss: 41038784.0000 Explore Prob: 0.0389\n",
      "Episode: 232 Total reward: -1.362740081580481 Training loss: 37028260.0000 Explore Prob: 0.0388\n",
      "Episode: 233 Total reward: -1.4834326077148816 Training loss: 467450848.0000 Explore Prob: 0.0387\n",
      "Episode: 234 Total reward: -1.5626652286515221 Training loss: 491698560.0000 Explore Prob: 0.0386\n",
      "Episode: 235 Total reward: -1.1219380442109068 Training loss: 76227088.0000 Explore Prob: 0.0385\n",
      "Episode: 236 Total reward: -1.6518659114963237 Training loss: 31641276.0000 Explore Prob: 0.0384\n",
      "Episode: 237 Total reward: -1.2135413330178935 Training loss: 554212928.0000 Explore Prob: 0.0383\n",
      "Episode: 238 Total reward: -1.370104723386879 Training loss: 62819308.0000 Explore Prob: 0.0382\n",
      "Episode: 239 Total reward: -1.5905738163537793 Training loss: 51601692.0000 Explore Prob: 0.0381\n",
      "Episode: 240 Total reward: -1.542162744394482 Training loss: 2230894592.0000 Explore Prob: 0.0380\n",
      "Episode: 241 Total reward: -1.0526185823733216 Training loss: 124750360.0000 Explore Prob: 0.0379\n",
      "Episode: 242 Total reward: -1.4634029262819213 Training loss: 2483809280.0000 Explore Prob: 0.0378\n",
      "Episode: 243 Total reward: -0.10533184037727872 Training loss: 460452992.0000 Explore Prob: 0.0376\n",
      "Episode: 244 Total reward: 0.433600556043929 Training loss: 77056352.0000 Explore Prob: 0.0375\n",
      "Episode: 245 Total reward: -1.4691797899952754 Training loss: 38324896.0000 Explore Prob: 0.0374\n",
      "Episode: 246 Total reward: -1.1871147194040048 Training loss: 101575216.0000 Explore Prob: 0.0373\n",
      "Episode: 247 Total reward: -1.4308197377752832 Training loss: 30445492.0000 Explore Prob: 0.0372\n",
      "Episode: 248 Total reward: -1.3414317229893802 Training loss: 23729160.0000 Explore Prob: 0.0371\n",
      "Episode: 249 Total reward: -1.448333906835628 Training loss: 31073974.0000 Explore Prob: 0.0370\n",
      "Episode: 250 Total reward: -1.6032705016302986 Training loss: 33597432.0000 Explore Prob: 0.0369\n",
      "Episode: 251 Total reward: -1.5168211224789896 Training loss: 2656714496.0000 Explore Prob: 0.0368\n",
      "Episode: 252 Total reward: -1.4083310085121212 Training loss: 22359362.0000 Explore Prob: 0.0367\n",
      "Episode: 253 Total reward: -1.362740081580481 Training loss: 336395456.0000 Explore Prob: 0.0366\n",
      "Episode: 254 Total reward: -1.2610193704484232 Training loss: 47451880.0000 Explore Prob: 0.0365\n",
      "Episode: 255 Total reward: -1.6470594160459033 Training loss: 2696712192.0000 Explore Prob: 0.0364\n",
      "Episode: 256 Total reward: -1.4068368355665046 Training loss: 41311276.0000 Explore Prob: 0.0363\n",
      "Episode: 257 Total reward: -1.6032705016302986 Training loss: 23505860.0000 Explore Prob: 0.0362\n",
      "Episode: 258 Total reward: -1.535310183086208 Training loss: 22463728.0000 Explore Prob: 0.0361\n",
      "Episode: 259 Total reward: -0.6606491080650527 Training loss: 2993716480.0000 Explore Prob: 0.0360\n",
      "Episode: 260 Total reward: 0.35194222485545057 Training loss: 46180432.0000 Explore Prob: 0.0358\n",
      "Episode: 261 Total reward: 0.5824866360293741 Training loss: 212046832.0000 Explore Prob: 0.0357\n",
      "Episode: 262 Total reward: 0.6880150464598789 Training loss: 17270760.0000 Explore Prob: 0.0355\n",
      "Episode: 263 Total reward: 0.6972484343829128 Training loss: 17845544.0000 Explore Prob: 0.0354\n",
      "Episode: 264 Total reward: 0.678438016008303 Training loss: 27423042.0000 Explore Prob: 0.0353\n",
      "Episode: 265 Total reward: 0.28416256994314404 Training loss: 2016529408.0000 Explore Prob: 0.0351\n",
      "Episode: 266 Total reward: 0.2518396915106572 Training loss: 31614000.0000 Explore Prob: 0.0350\n",
      "Episode: 267 Total reward: 1.1038146266863929 Training loss: 1756540288.0000 Explore Prob: 0.0349\n",
      "Episode: 268 Total reward: 8.731192990227886 Training loss: 1078744704.0000 Explore Prob: 0.0345\n",
      "Episode: 269 Total reward: 8.024040780489399 Training loss: 12555768.0000 Explore Prob: 0.0342\n",
      "Episode: 270 Total reward: 5.584523275330229 Training loss: 22387238.0000 Explore Prob: 0.0340\n",
      "Episode: 271 Total reward: 4.602285443172206 Training loss: 821961536.0000 Explore Prob: 0.0338\n",
      "Episode: 272 Total reward: 8.002171768213662 Training loss: 325298848.0000 Explore Prob: 0.0335\n",
      "Episode: 273 Total reward: 9.080077296036233 Training loss: 1189135360.0000 Explore Prob: 0.0332\n",
      "Episode: 274 Total reward: 9.07046862050425 Training loss: 653162304.0000 Explore Prob: 0.0329\n",
      "Episode: 275 Total reward: 5.414798366147311 Training loss: 15885192.0000 Explore Prob: 0.0327\n",
      "Episode: 276 Total reward: 5.322254488690688 Training loss: 13732312.0000 Explore Prob: 0.0325\n",
      "Episode: 277 Total reward: 3.7762010370188173 Training loss: 21082368.0000 Explore Prob: 0.0323\n",
      "Episode: 278 Total reward: 4.87021675113971 Training loss: 9288434.0000 Explore Prob: 0.0321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 279 Total reward: 2.8013879497124328 Training loss: 9975637.0000 Explore Prob: 0.0320\n",
      "Episode: 280 Total reward: 10.043863854147887 Training loss: 5243420.0000 Explore Prob: 0.0317\n",
      "Episode: 281 Total reward: 6.112169893883521 Training loss: 82644432.0000 Explore Prob: 0.0315\n",
      "Episode: 282 Total reward: 6.615002681978318 Training loss: 10712052.0000 Explore Prob: 0.0312\n",
      "Episode: 283 Total reward: 11.79085299081563 Training loss: 602441344.0000 Explore Prob: 0.0309\n",
      "Episode: 284 Total reward: 7.1551159520133 Training loss: 3233838.0000 Explore Prob: 0.0307\n",
      "Episode: 285 Total reward: 13.144218470444756 Training loss: 4077110.5000 Explore Prob: 0.0303\n",
      "Episode: 286 Total reward: 11.464669684986792 Training loss: 355891712.0000 Explore Prob: 0.0300\n",
      "Episode: 287 Total reward: 6.587616790414951 Training loss: 2651339.0000 Explore Prob: 0.0298\n",
      "Episode: 288 Total reward: -1.4755767520191987 Training loss: 48287636.0000 Explore Prob: 0.0297\n",
      "Episode: 289 Total reward: -1.1625091813427806 Training loss: 2978987.7500 Explore Prob: 0.0296\n",
      "Episode: 290 Total reward: -1.6520973434843083 Training loss: 1886515.7500 Explore Prob: 0.0295\n",
      "Episode: 291 Total reward: -1.2610193704484232 Training loss: 3359207.0000 Explore Prob: 0.0295\n",
      "Episode: 292 Total reward: -1.4755767520191987 Training loss: 15757392.0000 Explore Prob: 0.0294\n",
      "Episode: 293 Total reward: -1.5209437632479885 Training loss: 2998568.2500 Explore Prob: 0.0293\n",
      "Episode: 294 Total reward: -1.6032705016302986 Training loss: 1464583.2500 Explore Prob: 0.0293\n",
      "Episode: 295 Total reward: -1.3589491103542959 Training loss: 21973250.0000 Explore Prob: 0.0292\n",
      "Episode: 296 Total reward: -1.362740081580481 Training loss: 1023134.3750 Explore Prob: 0.0291\n",
      "Episode: 297 Total reward: -1.433183115636385 Training loss: 1401946.6250 Explore Prob: 0.0290\n",
      "Episode: 298 Total reward: -1.6032705016302986 Training loss: 107765600.0000 Explore Prob: 0.0290\n",
      "Episode: 299 Total reward: 5.938063896067978 Training loss: 134277808.0000 Explore Prob: 0.0288\n",
      "Episode: 300 Total reward: 3.1986274319382044 Training loss: 1558172.0000 Explore Prob: 0.0286\n",
      "Episode: 301 Total reward: -1.4691797899952754 Training loss: 2655937.0000 Explore Prob: 0.0286\n",
      "Episode: 302 Total reward: -1.4634029262819213 Training loss: 1726172.0000 Explore Prob: 0.0285\n",
      "Episode: 303 Total reward: -1.564019273897355 Training loss: 143967728.0000 Explore Prob: 0.0284\n",
      "Episode: 304 Total reward: -1.3419112867152019 Training loss: 597982.1250 Explore Prob: 0.0284\n",
      "Episode: 305 Total reward: -1.4755767520191987 Training loss: 84088776.0000 Explore Prob: 0.0283\n",
      "Episode: 306 Total reward: -1.6032705016302986 Training loss: 54506968.0000 Explore Prob: 0.0282\n",
      "Episode: 307 Total reward: -1.4755767520191987 Training loss: 865457.7500 Explore Prob: 0.0281\n",
      "Episode: 308 Total reward: -1.5194880904884445 Training loss: 602722.6250 Explore Prob: 0.0281\n",
      "Episode: 309 Total reward: -1.564019273897355 Training loss: 818625.8750 Explore Prob: 0.0280\n",
      "Episode: 310 Total reward: -1.5626652286515221 Training loss: 68077544.0000 Explore Prob: 0.0279\n",
      "Episode: 311 Total reward: -1.5690572013357589 Training loss: 1034938.0625 Explore Prob: 0.0279\n",
      "Episode: 312 Total reward: -1.350825199134851 Training loss: 899109.5000 Explore Prob: 0.0278\n",
      "Episode: 313 Total reward: -1.2761162071700913 Training loss: 11675300.0000 Explore Prob: 0.0277\n",
      "Episode: 314 Total reward: -1.357309882102393 Training loss: 480087.8750 Explore Prob: 0.0277\n",
      "Episode: 315 Total reward: -1.324467690359122 Training loss: 198655328.0000 Explore Prob: 0.0276\n",
      "Episode: 316 Total reward: -1.7437937070765046 Training loss: 2186763.7500 Explore Prob: 0.0275\n",
      "Episode: 317 Total reward: -1.6819633277226174 Training loss: 628460.5625 Explore Prob: 0.0275\n",
      "Episode: 318 Total reward: -1.7479134306995192 Training loss: 8745915.0000 Explore Prob: 0.0274\n",
      "Episode: 319 Total reward: -1.741355030316512 Training loss: 496939.4375 Explore Prob: 0.0274\n",
      "Episode: 320 Total reward: -1.8909463576925245 Training loss: 19740724.0000 Explore Prob: 0.0273\n",
      "Episode: 321 Total reward: -1.8962098253675377 Training loss: 504243.6562 Explore Prob: 0.0272\n",
      "Episode: 322 Total reward: -1.9045482824220503 Training loss: 3044993.2500 Explore Prob: 0.0272\n",
      "Episode: 323 Total reward: -2.0326003966213215 Training loss: 421526.2812 Explore Prob: 0.0271\n",
      "Episode: 324 Total reward: -1.7039926768546718 Training loss: 2640168.2500 Explore Prob: 0.0270\n",
      "Episode: 325 Total reward: -2.314626410459189 Training loss: 18109698.0000 Explore Prob: 0.0270\n",
      "Episode: 326 Total reward: -2.2424668768674905 Training loss: 36686600.0000 Explore Prob: 0.0269\n",
      "Episode: 327 Total reward: -2.262540745543593 Training loss: 5073893.5000 Explore Prob: 0.0269\n",
      "Episode: 328 Total reward: -1.8709837444204196 Training loss: 352565.5000 Explore Prob: 0.0268\n",
      "Episode: 329 Total reward: -2.2829292488494346 Training loss: 5043001.5000 Explore Prob: 0.0268\n",
      "Episode: 330 Total reward: -2.2012649672205535 Training loss: 10065659.0000 Explore Prob: 0.0267\n",
      "Episode: 331 Total reward: -2.153065084430306 Training loss: 9065762.0000 Explore Prob: 0.0266\n",
      "Episode: 332 Total reward: -2.0080142609284075 Training loss: 21211564.0000 Explore Prob: 0.0266\n",
      "Episode: 333 Total reward: -2.302151143896091 Training loss: 7193891.5000 Explore Prob: 0.0265\n",
      "Episode: 334 Total reward: -2.251615263067355 Training loss: 19055328.0000 Explore Prob: 0.0265\n",
      "Episode: 335 Total reward: -2.07219504087966 Training loss: 400986.6250 Explore Prob: 0.0264\n",
      "Episode: 336 Total reward: -2.451152545195418 Training loss: 480107.0000 Explore Prob: 0.0264\n",
      "Episode: 337 Total reward: -2.451152545195418 Training loss: 8949468.0000 Explore Prob: 0.0263\n",
      "Episode: 338 Total reward: -2.0838669291371756 Training loss: 391256.0625 Explore Prob: 0.0263\n",
      "Episode: 339 Total reward: -2.333269661340686 Training loss: 336259.9688 Explore Prob: 0.0262\n",
      "Episode: 340 Total reward: -2.353892626834229 Training loss: 762425.2500 Explore Prob: 0.0261\n",
      "Episode: 341 Total reward: -2.3044553772468688 Training loss: 29231958.0000 Explore Prob: 0.0261\n",
      "Episode: 342 Total reward: -1.9978286155938962 Training loss: 9434992.0000 Explore Prob: 0.0260\n",
      "Episode: 343 Total reward: -2.494240811255914 Training loss: 637487.1250 Explore Prob: 0.0260\n",
      "Episode: 344 Total reward: -2.4084522498760568 Training loss: 425040.6250 Explore Prob: 0.0259\n",
      "Episode: 345 Total reward: -2.4015966709361036 Training loss: 701587.8125 Explore Prob: 0.0259\n",
      "Episode: 346 Total reward: -2.594776584277804 Training loss: 1347818.2500 Explore Prob: 0.0258\n",
      "Episode: 347 Total reward: -2.0470124505676126 Training loss: 461235.9688 Explore Prob: 0.0258\n",
      "Episode: 348 Total reward: -2.621591375633937 Training loss: 295734.4375 Explore Prob: 0.0257\n",
      "Episode: 349 Total reward: -2.621591375633937 Training loss: 318377.6250 Explore Prob: 0.0257\n",
      "Episode: 350 Total reward: -2.4751582508492636 Training loss: 640296.4375 Explore Prob: 0.0256\n",
      "Episode: 351 Total reward: -2.4237194331084604 Training loss: 1551023.0000 Explore Prob: 0.0256\n",
      "Episode: 352 Total reward: -2.69318563760328 Training loss: 539118.0000 Explore Prob: 0.0255\n",
      "Episode: 353 Total reward: -2.4165873320662405 Training loss: 501102.6562 Explore Prob: 0.0255\n",
      "Episode: 354 Total reward: -2.2193864482705052 Training loss: 604434.5000 Explore Prob: 0.0254\n",
      "Episode: 355 Total reward: -2.697111796801663 Training loss: 510416.4375 Explore Prob: 0.0254\n",
      "Episode: 356 Total reward: -2.742116166443333 Training loss: 445427.7500 Explore Prob: 0.0253\n",
      "Episode: 357 Total reward: -2.673574807999632 Training loss: 530784.2500 Explore Prob: 0.0253\n",
      "Episode: 358 Total reward: -2.409168866448514 Training loss: 422246.3750 Explore Prob: 0.0252\n",
      "Episode: 359 Total reward: -2.544409353965049 Training loss: 522463.4375 Explore Prob: 0.0252\n",
      "Episode: 360 Total reward: -2.542708553770949 Training loss: 733972.3750 Explore Prob: 0.0251\n",
      "Episode: 361 Total reward: -2.5124324762029744 Training loss: 257421.8125 Explore Prob: 0.0251\n",
      "Episode: 362 Total reward: -2.2804967756502714 Training loss: 548400.7500 Explore Prob: 0.0250\n",
      "Episode: 363 Total reward: -2.7114672850222306 Training loss: 484532.5625 Explore Prob: 0.0250\n",
      "Episode: 364 Total reward: -2.5160630294211 Training loss: 305479.5938 Explore Prob: 0.0249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 365 Total reward: -2.611863456168622 Training loss: 356788.1250 Explore Prob: 0.0249\n",
      "Episode: 366 Total reward: -2.823855494557991 Training loss: 10059250.0000 Explore Prob: 0.0249\n",
      "Episode: 367 Total reward: -2.611196785961284 Training loss: 625955.7500 Explore Prob: 0.0248\n",
      "Episode: 368 Total reward: -2.7321424283040967 Training loss: 29994352.0000 Explore Prob: 0.0248\n",
      "Episode: 369 Total reward: -2.8193656452451696 Training loss: 443037.6875 Explore Prob: 0.0247\n",
      "Episode: 370 Total reward: -2.611196785961284 Training loss: 545037.7500 Explore Prob: 0.0247\n",
      "Episode: 371 Total reward: -2.694812561594278 Training loss: 352206.9375 Explore Prob: 0.0246\n",
      "Episode: 372 Total reward: -2.703754591255694 Training loss: 328037.3750 Explore Prob: 0.0246\n",
      "Episode: 373 Total reward: -2.857550135094965 Training loss: 391503.4375 Explore Prob: 0.0245\n",
      "Episode: 374 Total reward: -2.687004991321505 Training loss: 336376.2500 Explore Prob: 0.0245\n",
      "Episode: 375 Total reward: -2.990159052394393 Training loss: 8705420.0000 Explore Prob: 0.0245\n",
      "Episode: 376 Total reward: -2.8054796429758166 Training loss: 470478.5312 Explore Prob: 0.0244\n",
      "Episode: 377 Total reward: -2.871163039537959 Training loss: 690402.2500 Explore Prob: 0.0244\n",
      "Episode: 378 Total reward: -2.4437842873578726 Training loss: 400472.8750 Explore Prob: 0.0243\n",
      "Episode: 379 Total reward: -2.6868562108229455 Training loss: 385166.3438 Explore Prob: 0.0243\n",
      "Episode: 380 Total reward: -2.4526685075397077 Training loss: 313408.4062 Explore Prob: 0.0242\n",
      "Episode: 381 Total reward: -2.5980024292525994 Training loss: 271376.4375 Explore Prob: 0.0242\n",
      "Episode: 382 Total reward: -2.391284035531846 Training loss: 564571.6250 Explore Prob: 0.0242\n",
      "Episode: 383 Total reward: -2.4024196790334775 Training loss: 537016.5625 Explore Prob: 0.0241\n",
      "Episode: 384 Total reward: -2.557534088287995 Training loss: 329850.1875 Explore Prob: 0.0241\n",
      "Episode: 385 Total reward: -2.4719460420040225 Training loss: 407521.3438 Explore Prob: 0.0240\n",
      "Episode: 386 Total reward: -2.314616715839306 Training loss: 565936.1250 Explore Prob: 0.0240\n",
      "Episode: 387 Total reward: -1.9404848348491641 Training loss: 432473.4375 Explore Prob: 0.0240\n",
      "Episode: 388 Total reward: -2.365428610415154 Training loss: 446883.3750 Explore Prob: 0.0239\n",
      "Episode: 389 Total reward: -2.1323695462466827 Training loss: 10136253.0000 Explore Prob: 0.0239\n",
      "Episode: 390 Total reward: -2.1049963131325553 Training loss: 673937.6875 Explore Prob: 0.0238\n",
      "Episode: 391 Total reward: -2.0375040715495465 Training loss: 650193.1250 Explore Prob: 0.0238\n",
      "Episode: 392 Total reward: -2.104411001117298 Training loss: 528376.9375 Explore Prob: 0.0238\n",
      "Episode: 393 Total reward: -1.9096253430645453 Training loss: 795372.7500 Explore Prob: 0.0237\n",
      "Episode: 394 Total reward: -2.035871075640878 Training loss: 299577.5625 Explore Prob: 0.0237\n",
      "Episode: 395 Total reward: -2.0160077272348396 Training loss: 449142.5625 Explore Prob: 0.0236\n",
      "Episode: 396 Total reward: -1.9235698237562693 Training loss: 302055.9375 Explore Prob: 0.0236\n",
      "Episode: 397 Total reward: -1.9211787412663033 Training loss: 726720.4375 Explore Prob: 0.0235\n",
      "Episode: 398 Total reward: -1.858036641895945 Training loss: 412465.7812 Explore Prob: 0.0235\n",
      "Episode: 399 Total reward: -1.8115345768127802 Training loss: 546410.8125 Explore Prob: 0.0235\n",
      "Episode: 400 Total reward: -1.8063825270580047 Training loss: 481156.1250 Explore Prob: 0.0234\n",
      "Episode: 401 Total reward: -1.6864114995968236 Training loss: 436589.0000 Explore Prob: 0.0234\n",
      "Episode: 402 Total reward: -1.6108716981725035 Training loss: 677115.4375 Explore Prob: 0.0233\n",
      "Episode: 403 Total reward: -0.9037649370754591 Training loss: 578847.0625 Explore Prob: 0.0233\n",
      "Episode: 404 Total reward: -1.0484033567158568 Training loss: 260351.6875 Explore Prob: 0.0232\n",
      "Episode: 405 Total reward: -0.9176015389446203 Training loss: 409680.0625 Explore Prob: 0.0232\n",
      "Episode: 406 Total reward: -1.1455034538613873 Training loss: 7920703.5000 Explore Prob: 0.0231\n",
      "Episode: 407 Total reward: -1.308529799084475 Training loss: 549190.6250 Explore Prob: 0.0231\n",
      "Episode: 408 Total reward: -1.335707581875061 Training loss: 266294.3750 Explore Prob: 0.0230\n",
      "Episode: 409 Total reward: -1.1397914749548146 Training loss: 510485.0625 Explore Prob: 0.0229\n",
      "Episode: 410 Total reward: -1.2536341189756326 Training loss: 299953.6875 Explore Prob: 0.0229\n",
      "Episode: 411 Total reward: -0.90620699053005 Training loss: 528762.9375 Explore Prob: 0.0228\n",
      "Episode: 412 Total reward: -1.0004300485755935 Training loss: 590369.1250 Explore Prob: 0.0228\n",
      "Episode: 413 Total reward: -1.0425962602772219 Training loss: 481314.9375 Explore Prob: 0.0227\n",
      "Episode: 414 Total reward: -1.1551869470788203 Training loss: 720581.7500 Explore Prob: 0.0226\n",
      "Episode: 415 Total reward: 1.342861265268464 Training loss: 347344.8438 Explore Prob: 0.0225\n",
      "Episode: 416 Total reward: 1.3888876723939925 Training loss: 449708.8438 Explore Prob: 0.0224\n",
      "Episode: 417 Total reward: 0.8075181724096445 Training loss: 347280.0000 Explore Prob: 0.0223\n",
      "Episode: 418 Total reward: -0.05035633182884333 Training loss: 793932.0000 Explore Prob: 0.0221\n",
      "Episode: 419 Total reward: -1.7462130349917897 Training loss: 236412.9375 Explore Prob: 0.0219\n",
      "Episode: 420 Total reward: -4.371200055793591 Training loss: 301331.3438 Explore Prob: 0.0217\n",
      "Episode: 421 Total reward: -18.443268291959118 Training loss: 444207.1875 Explore Prob: 0.0215\n",
      "Episode: 422 Total reward: -9.634007041325635 Training loss: 463576.8750 Explore Prob: 0.0213\n",
      "Episode: 423 Total reward: -9.266211706726706 Training loss: 380780.2812 Explore Prob: 0.0212\n",
      "Episode: 424 Total reward: -7.121605214654659 Training loss: 360753.6250 Explore Prob: 0.0211\n",
      "Episode: 425 Total reward: -6.300424122833778 Training loss: 382379.9688 Explore Prob: 0.0210\n",
      "Episode: 426 Total reward: -3.8402654648025587 Training loss: 292480.0625 Explore Prob: 0.0209\n",
      "Episode: 427 Total reward: -2.494164242287894 Training loss: 503043.0938 Explore Prob: 0.0208\n",
      "Episode: 428 Total reward: -4.354027416462237 Training loss: 191017.5625 Explore Prob: 0.0207\n",
      "Episode: 429 Total reward: -4.699119094102428 Training loss: 291078.8125 Explore Prob: 0.0205\n",
      "Episode: 430 Total reward: -4.184728461815747 Training loss: 380897.2500 Explore Prob: 0.0204\n",
      "Episode: 431 Total reward: -4.662572356988111 Training loss: 296129.9375 Explore Prob: 0.0203\n",
      "Episode: 432 Total reward: -4.140689490160587 Training loss: 411172.5625 Explore Prob: 0.0202\n",
      "Episode: 433 Total reward: -3.7360505022913255 Training loss: 356609.5625 Explore Prob: 0.0201\n",
      "Episode: 434 Total reward: -2.972291084334574 Training loss: 696910.0000 Explore Prob: 0.0200\n",
      "Episode: 435 Total reward: -3.873379276707159 Training loss: 183579.6562 Explore Prob: 0.0199\n",
      "Episode: 436 Total reward: -4.316509684166464 Training loss: 296706.3438 Explore Prob: 0.0197\n",
      "Episode: 437 Total reward: 6.0964009837686834 Training loss: 412410.5000 Explore Prob: 0.0195\n",
      "Episode: 438 Total reward: 3.882229376845936 Training loss: 171121.7812 Explore Prob: 0.0194\n",
      "Episode: 439 Total reward: 1.084370870837639 Training loss: 245590.5469 Explore Prob: 0.0193\n",
      "Episode: 440 Total reward: 1.161522802853147 Training loss: 219443.4062 Explore Prob: 0.0192\n",
      "Episode: 441 Total reward: 1.252057102455736 Training loss: 540163.1250 Explore Prob: 0.0192\n",
      "Episode: 442 Total reward: 1.016215483182651 Training loss: 149592.7188 Explore Prob: 0.0191\n",
      "Episode: 443 Total reward: 0.928160590109614 Training loss: 235834.2656 Explore Prob: 0.0190\n",
      "Episode: 444 Total reward: 0.9656740470231547 Training loss: 250120.4688 Explore Prob: 0.0189\n",
      "Episode: 445 Total reward: 0.8874722406127988 Training loss: 571537.0000 Explore Prob: 0.0189\n",
      "Episode: 446 Total reward: 1.2888716202852797 Training loss: 113978.8750 Explore Prob: 0.0188\n",
      "Episode: 447 Total reward: 0.8382627908274138 Training loss: 127541.9844 Explore Prob: 0.0187\n",
      "Episode: 448 Total reward: 1.3872530736631727 Training loss: 216547.0469 Explore Prob: 0.0187\n",
      "Episode: 449 Total reward: 1.0317812210839632 Training loss: 133145.0938 Explore Prob: 0.0186\n",
      "Episode: 450 Total reward: 1.0318370339228233 Training loss: 71239.5312 Explore Prob: 0.0185\n",
      "Episode: 451 Total reward: 1.07556547518449 Training loss: 418685.3438 Explore Prob: 0.0184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 452 Total reward: 1.111713788581437 Training loss: 104444.4062 Explore Prob: 0.0184\n",
      "Episode: 453 Total reward: 2.100765763498919 Training loss: 364753.9688 Explore Prob: 0.0183\n",
      "Episode: 454 Total reward: 1.4740193241633655 Training loss: 900996.7500 Explore Prob: 0.0182\n",
      "Episode: 455 Total reward: 1.7843435620660677 Training loss: 117538.2031 Explore Prob: 0.0181\n",
      "Episode: 456 Total reward: 3.17030583493334 Training loss: 379133.0312 Explore Prob: 0.0181\n",
      "Episode: 457 Total reward: 2.8644180096804046 Training loss: 103390.3125 Explore Prob: 0.0180\n",
      "Episode: 458 Total reward: 3.3958867571480944 Training loss: 101369.5625 Explore Prob: 0.0179\n",
      "Episode: 459 Total reward: 2.9621279244582452 Training loss: 194869.7031 Explore Prob: 0.0179\n",
      "Episode: 460 Total reward: 3.277993872713315 Training loss: 68811.2188 Explore Prob: 0.0178\n",
      "Episode: 461 Total reward: 2.5662687082796345 Training loss: 47597.6250 Explore Prob: 0.0178\n",
      "Episode: 462 Total reward: 2.935259948369405 Training loss: 78176.0156 Explore Prob: 0.0177\n",
      "Episode: 463 Total reward: 3.2895487439514173 Training loss: 47999.5547 Explore Prob: 0.0177\n",
      "Episode: 464 Total reward: 2.0175831069835777 Training loss: 401248.4375 Explore Prob: 0.0176\n",
      "Episode: 465 Total reward: 1.7345504205239817 Training loss: 63295.0938 Explore Prob: 0.0175\n",
      "Episode: 466 Total reward: 1.8314191172551202 Training loss: 166083.0469 Explore Prob: 0.0174\n",
      "Episode: 467 Total reward: 1.7065685309672678 Training loss: 71841.3906 Explore Prob: 0.0174\n",
      "Episode: 468 Total reward: 1.8782350149572087 Training loss: 45643.4219 Explore Prob: 0.0173\n",
      "Episode: 469 Total reward: 1.6786018632372508 Training loss: 62924.6445 Explore Prob: 0.0172\n",
      "Episode: 470 Total reward: 2.2035803854032423 Training loss: 68650.5781 Explore Prob: 0.0172\n",
      "Episode: 471 Total reward: 2.20308007451397 Training loss: 230646.8438 Explore Prob: 0.0171\n",
      "Episode: 472 Total reward: 2.418678871199247 Training loss: 344222.2188 Explore Prob: 0.0170\n",
      "Episode: 473 Total reward: 1.3882607310138413 Training loss: 36227.6914 Explore Prob: 0.0170\n",
      "Episode: 474 Total reward: 2.2365898247341836 Training loss: 307924.3125 Explore Prob: 0.0169\n",
      "Episode: 475 Total reward: 1.7895343101037924 Training loss: 94480.0156 Explore Prob: 0.0168\n",
      "Episode: 476 Total reward: 1.9930364017322142 Training loss: 50911.4453 Explore Prob: 0.0168\n",
      "Episode: 477 Total reward: 1.9302187420186865 Training loss: 65918.6172 Explore Prob: 0.0167\n",
      "Episode: 478 Total reward: 1.8887848801484632 Training loss: 243449.2031 Explore Prob: 0.0167\n",
      "Episode: 479 Total reward: 2.550837968364909 Training loss: 29480.6035 Explore Prob: 0.0166\n",
      "Episode: 480 Total reward: 3.0161278671873686 Training loss: 62359.0977 Explore Prob: 0.0165\n",
      "Episode: 481 Total reward: 2.1451088871174258 Training loss: 72046.8984 Explore Prob: 0.0165\n",
      "Episode: 482 Total reward: 2.677224842444024 Training loss: 159268.5156 Explore Prob: 0.0164\n",
      "Episode: 483 Total reward: 2.1466586922726827 Training loss: 179460.0156 Explore Prob: 0.0163\n",
      "Episode: 484 Total reward: 2.45948636614207 Training loss: 56834.5312 Explore Prob: 0.0163\n",
      "Episode: 485 Total reward: 3.1431477245382986 Training loss: 75939.0234 Explore Prob: 0.0162\n",
      "Episode: 486 Total reward: 3.469633823790203 Training loss: 192007.7812 Explore Prob: 0.0161\n",
      "Episode: 487 Total reward: 3.1342820641656317 Training loss: 16445.6680 Explore Prob: 0.0161\n",
      "Episode: 488 Total reward: 2.738122919855108 Training loss: 56301.7930 Explore Prob: 0.0160\n",
      "Episode: 489 Total reward: 4.033868014754148 Training loss: 30329.7227 Explore Prob: 0.0159\n",
      "Episode: 490 Total reward: 3.0426239326285343 Training loss: 82412.3203 Explore Prob: 0.0159\n",
      "Episode: 491 Total reward: 2.6376317133132456 Training loss: 41788.3672 Explore Prob: 0.0158\n",
      "Episode: 492 Total reward: 2.6092189976162596 Training loss: 11960.8936 Explore Prob: 0.0157\n",
      "Episode: 493 Total reward: 4.021599401687201 Training loss: 11215.5840 Explore Prob: 0.0157\n",
      "Episode: 494 Total reward: 3.6218078458535525 Training loss: 31039.4512 Explore Prob: 0.0156\n",
      "Episode: 495 Total reward: 3.404748982453736 Training loss: 40897.4219 Explore Prob: 0.0155\n",
      "Episode: 496 Total reward: 1.5128427847796997 Training loss: 11877.1484 Explore Prob: 0.0155\n",
      "Episode: 497 Total reward: 4.50455490323344 Training loss: 90435.4062 Explore Prob: 0.0154\n",
      "Episode: 498 Total reward: 11.347724350196888 Training loss: 44435.0898 Explore Prob: 0.0153\n",
      "Episode: 499 Total reward: 3.789292280687586 Training loss: 24189.0547 Explore Prob: 0.0152\n"
     ]
    }
   ],
   "source": [
    "# Now train with experience\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    decay_step = 0\n",
    "    for episode in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        step =0\n",
    "        while step < max_steps:\n",
    "            decay_step +=1\n",
    "#             env.render(mode='human')\n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_prob = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*decay_step)\n",
    "            \n",
    "            if explore_prob > np.random.rand() : # return [0,1)\n",
    "                # random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_:state.reshape((1, *state.shape))}\n",
    "                Q_pred = sess.run(mainQN.output , feed_dict= feed)\n",
    "                action = np.argmax(Q_pred)\n",
    "                \n",
    "            # Take action, get new_state and reward\n",
    "            next_state , reward , done, _ = env.step(action)\n",
    "            \n",
    "            total_reward +=reward\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                step = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(episode),\n",
    "                     'Total reward: {}'.format(total_reward),\n",
    "                     'Training loss: {:.4f}'.format(loss),\n",
    "                     'Explore Prob: {:.4f}'.format(explore_prob))\n",
    "                \n",
    "                rewards_list.append((episode , total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action,reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "                \n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                step +=1\n",
    "                \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states_mb = np.array([each[0] for each in batch])\n",
    "            actions_mb = np.array([each[1] for each in batch])\n",
    "            rewards_mb = np.array([each[2] for each in batch])\n",
    "            next_states_mb = np.array([each[3] for each in batch])\n",
    "            \n",
    "            ### TRAINING NETWORK\n",
    "            targetQs = sess.run(mainQN.output, feed_dict={mainQN.inputs_:next_states_mb})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends \n",
    "            episode_ends = (next_states_mb==np.zeros(states_mb[0].shape)).all(axis=1)\n",
    "            \n",
    "            targetQs[episode_ends]  = (0, 0 , 0 , 0 , 0 , 0, 0 ,0 ,0 )\n",
    "            \n",
    "            targets = rewards_mb + gamma*np.max(targetQs, axis=1)\n",
    "            \n",
    "            loss , _ = sess.run([mainQN.loss , mainQN.optimizer],\n",
    "                               feed_dict={mainQN.inputs_:states_mb,\n",
    "                                         mainQN.targetQs_: targets,\n",
    "                                         mainQN.actions_: actions_mb})\n",
    "            \n",
    "            \n",
    "    \n",
    "    saver.save(sess, \"checkpoints/cartpole{}.ckpt\".format(train_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Reward')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f768435fda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cartpole500.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for ep in range(1, test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "#             env.render() \n",
    "            \n",
    "            # Get action from Q-network\n",
    "            feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "            Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "#                 print('Episode: {}'.format(ep),\n",
    "#                      'Total reward: {}'.format(total_reward),\n",
    "#                      'Training loss: {:.4f}'.format(loss),\n",
    "#                      'Explore Prob: {:.4f}'.format(explore_prob))\n",
    "                \n",
    "\n",
    "            else:\n",
    "                state = next_state\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
